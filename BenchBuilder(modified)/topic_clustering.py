import json
import argparse
import os


import torch
from tqdm import tqdm
import numpy as np

from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance

import openai
import tiktoken
from bertopic.representation import OpenAI

import time
import random

def run(args):
    print("æ­£åœ¨åŠ è½½ä¸­æ–‡embeddingæ¨¡å‹...")
    try:
        embedding_model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
        print("æ¨¡å‹åŠ è½½å®Œæˆï¼")
    except Exception as e:
        print(f"âŒ Embeddingæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("ğŸ”§ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
        try:
            embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
            print("âœ… å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e2:
            print(f"âŒ å¤‡ç”¨æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
            raise
    
    embeddings_file = f"{args.output_dir}/embeddings.npy"
    post_process_file = f"{args.output_dir}/post_process_convs.json"
    
    if args.embedding_file is not None:
        embeddings = np.load(args.embedding_file)
        if args.post_process_conv is not None:
            all_convs = json.load(open(args.post_process_conv))
        else:
            raise ValueError("Please provide post process conv file")

        convs = []
        for row in all_convs:
            convs.append(row["post_process_conv"])
            
    elif os.path.exists(embeddings_file) and os.path.exists(post_process_file):
        
        try:
            embeddings = np.load(embeddings_file)
            all_convs_new = json.load(open(post_process_file, 'r', encoding='utf-8'))
            convs = [row["post_process_conv"] for row in all_convs_new]
            
        
            if len(convs) != embeddings.shape[0]:
                print(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´: å¯¹è¯{len(convs)} vs embeddings{embeddings.shape[0]}")
                min_len = min(len(convs), embeddings.shape[0])
                convs = convs[:min_len]
                embeddings = embeddings[:min_len]
                print(f"å·²è°ƒæ•´ä¸ºä¸€è‡´é•¿åº¦: {min_len}")
            
        except Exception as e:
            print(f"æ¢å¤æ•°æ®å¤±è´¥: {e}")
            print("å°†é‡æ–°å¤„ç†åŸå§‹æ•°æ®...")
            if os.path.exists(embeddings_file):
                os.remove(embeddings_file)
            if os.path.exists(post_process_file):
                os.remove(post_process_file)
            embeddings = None
            convs = None
            
    else:
        embeddings = None
        convs = None
    
    if embeddings is None or convs is None:
        if not args.conv_file:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯æ¢å¤çš„æ•°æ®ï¼Œè¯·æä¾› --conv-file å‚æ•°")
            
        all_convs = json.load(open(args.conv_file))

        if args.first_n is not None:
            all_convs = all_convs[:args.first_n]

        all_convs_new = []
        convs = []
        for row in all_convs:
            try:
                contents_data = json.loads(row["contents"])
                
                conv = ""
                for item in contents_data:
                    if item.get("type") == "text" and "content" in item:
                        conv += f"{item['content']}\n"
                
                conv = conv.replace("<|endoftext|>", "<| endoftext |>")
                conv = conv.strip()
                
                if len(conv) <= 32:
                    continue
                    
                conv_truncated = conv[:10000]
                convs.append(conv_truncated)
                
                row_copy = row.copy()
                row_copy["post_process_conv"] = conv_truncated
                all_convs_new.append(row_copy)
                
            except (json.JSONDecodeError, KeyError, TypeError) as e:
                print(f"è·³è¿‡æ— æ•ˆè®°å½• ID {row.get('id', 'unknown')}: {e}")
                continue

        print(f"æˆåŠŸå¤„ç† {len(convs)} æ¡å¯¹è¯")

        print("ä¿å­˜å¤„ç†åçš„å¯¹è¯æ•°æ®...")
        try:
            with open(f"{args.output_dir}/post_process_convs.json", "w", encoding="utf-8") as f:
                json.dump(all_convs_new, f, indent=4, ensure_ascii=False)
            print("å¯¹è¯æ•°æ®ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"å¯¹è¯æ•°æ®ä¿å­˜å¤±è´¥: {e}")

        print("ğŸ“„ å¼€å§‹ç”Ÿæˆembeddings...")
        batch_size = 32
        embeddings = []
        
        start_batch = 0
        temp_embeddings_file = f"{args.output_dir}/embeddings_temp.npy"
        
        if os.path.exists(temp_embeddings_file):
            try:
                temp_embeddings = np.load(temp_embeddings_file)
                start_batch = temp_embeddings.shape[0] // batch_size
                embeddings = [temp_embeddings[i:i+batch_size] for i in range(0, temp_embeddings.shape[0], batch_size)]
                print(f"å‘ç°ä¸´æ—¶embeddingsï¼Œä»ç¬¬ {start_batch+1} æ‰¹æ¬¡æ¢å¤...")
            except:
                print("ä¸´æ—¶embeddingsæ–‡ä»¶æŸåï¼Œé‡æ–°å¼€å§‹...")
                start_batch = 0
                embeddings = []
        
        for i in tqdm(range(start_batch * batch_size, len(convs), batch_size), desc="ç”Ÿæˆembeddings"):
            convs_batch = convs[i : i + batch_size]
            try:
                batch_embeddings = embedding_model.encode(
                    convs_batch,
                    batch_size=batch_size,
                    show_progress_bar=False,
                    convert_to_numpy=True
                )
                embeddings.append(batch_embeddings)
                
                if (i // batch_size + 1) % 10 == 0:
                    current_embeddings = np.vstack(embeddings)
                    np.save(temp_embeddings_file, current_embeddings)
                    print(f"ä¸´æ—¶ä¿å­˜embeddingsè¿›åº¦: {current_embeddings.shape[0]}/{len(convs)}")
                
            except Exception as e:
                print(f"Embeddingç”Ÿæˆé”™è¯¯ (batch {i//batch_size + 1}): {e}")
                if embeddings:
                    current_embeddings = np.vstack(embeddings)
                    np.save(temp_embeddings_file, current_embeddings)
                    print(f"å·²ä¿å­˜å½“å‰è¿›åº¦: {current_embeddings.shape[0]} embeddings")
                raise
        
        embeddings = np.vstack(embeddings)
        
        np.save(f"{args.output_dir}/embeddings.npy", embeddings)
        if os.path.exists(temp_embeddings_file):
            os.remove(temp_embeddings_file)
        print(f"âœ… Embeddingså·²ä¿å­˜ï¼Œå½¢çŠ¶: {embeddings.shape}")

    print(f"ğŸ“Š æ€»å¯¹è¯æ•°: {len(convs)}")
        
    print("é…ç½®è¡¨ç¤ºæ¨¡å‹...")
    representation_model = {}
    
    try:
        keybert_model = KeyBERTInspired()
        representation_model["KeyBERT"] = keybert_model
        print("KeyBERTé…ç½®æˆåŠŸ")
    except Exception as e:
        print(f"KeyBERTé…ç½®å¤±è´¥: {e}")
    
    if hasattr(args, 'use_openai') and args.use_openai:
        try:
            tokenizer = tiktoken.encoding_for_model("gpt-4o-mini")
            
           YOUR_API_KEY = ""  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„OpenAI APIå¯†é’¥
            prompt = """
            æˆ‘æœ‰ä¸€ä¸ªä¸»é¢˜åŒ…å«ä»¥ä¸‹æ–‡æ¡£ï¼š
            [DOCUMENTS]
            è¿™ä¸ªä¸»é¢˜ç”±ä»¥ä¸‹å…³é”®è¯æè¿°ï¼š[KEYWORDS]

            åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè¯·æå–ä¸€ä¸ªç®€çŸ­ä½†é«˜åº¦æè¿°æ€§çš„ä¸»é¢˜æ ‡ç­¾ï¼Œæœ€å¤š5ä¸ªè¯ã€‚è¯·ç¡®ä¿æ ¼å¼å¦‚ä¸‹ï¼š
            ä¸»é¢˜ï¼š<ä¸»é¢˜æ ‡ç­¾>
            """
            
            openai_model = OpenAI(
                client, 
                model="openai/gpt-4o-mini",
                exponential_backoff=True, 
                chat=True, 
                prompt=prompt, 
                nr_docs=20,
                doc_length=200,
                tokenizer=tokenizer,
            )
            representation_model["OpenAI"] = openai_model
            print("OpenAIæ¨¡å‹é…ç½®æˆåŠŸ")
            
        except Exception as e:
            print(f"OpenAIé…ç½®å¤±è´¥: {e}")
            print("å°†ä»…ä½¿ç”¨KeyBERTè¿›è¡Œä¸»é¢˜è¡¨ç¤º...")
    
    if embedding_model is None:
        print("é‡æ–°åˆå§‹åŒ–embeddingæ¨¡å‹...")
        try:
            embedding_model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
        except:
            embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

    if not representation_model:
        print("ä½¿ç”¨é»˜è®¤KeyBERTè¡¨ç¤ºæ¨¡å‹...")
        representation_model = KeyBERTInspired()

    print("åˆ›å»ºBERTopicæ¨¡å‹...")
    try:
        topic_model = BERTopic(
            verbose=True,
            embedding_model=embedding_model,
            representation_model=representation_model,
            min_topic_size=args.min_topic_size,
        )
        print("BERTopicæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    except Exception as e:
        print(f"BERTopicåˆ›å»ºå¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨ç®€åŒ–é…ç½®...")
        topic_model = BERTopic(
            verbose=True,
            embedding_model=embedding_model,
            min_topic_size=args.min_topic_size,
        )
        print("ç®€åŒ–BERTopicæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    print("ğŸ“„ å¼€å§‹è®­ç»ƒBERTopicæ¨¡å‹...")
    try:
        topics, _ = topic_model.fit_transform(convs, embeddings)
        print(f"å‘ç° {len(topic_model.get_topic_info())} ä¸ªä¸»é¢˜")
    except Exception as e:
        print(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        print("å°è¯•ä½¿ç”¨æ›´ç®€å•çš„é…ç½®é‡æ–°è®­ç»ƒ...")
        
        simple_topic_model = BERTopic(
            verbose=True,
            embedding_model=embedding_model,
            min_topic_size=max(args.min_topic_size, 5),
        )
        topics, _ = simple_topic_model.fit_transform(convs, embeddings)
        topic_model = simple_topic_model
        print(f"ç®€åŒ–æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œå‘ç° {len(topic_model.get_topic_info())} ä¸ªä¸»é¢˜")

    print("å¤„ç†ç¦»ç¾¤å€¼...")
    try:
        new_topics = topic_model.reduce_outliers(convs, topics)
        print("ç¦»ç¾¤å€¼å¤„ç†å®Œæˆ")
    except Exception as e:
        print(f"ç¦»ç¾¤å€¼å¤„ç†å¤±è´¥: {e}")
        print("è·³è¿‡ç¦»ç¾¤å€¼å¤„ç†ï¼Œä½¿ç”¨åŸå§‹ä¸»é¢˜åˆ†é…...")
        new_topics = topics
    
    print("ä¿å­˜ä¸»é¢˜åˆ†é…ç»“æœ...")
    try:
        with open(f"{args.output_dir}/conv_topics.json", "w", encoding="utf-8") as f:
            json.dump(new_topics, f, default=str, ensure_ascii=False)
        print("ä¸»é¢˜åˆ†é…ç»“æœä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"ä¸»é¢˜åˆ†é…ç»“æœä¿å­˜å¤±è´¥: {e}")

    print("ä¿å­˜æ¨¡å‹...")
    try:
        topic_model.save(
            f"{args.output_dir}/model_dir", 
            serialization="pickle", 
            save_ctfidf=True, 
            save_embedding_model=False
        )
        print("æ¨¡å‹ä¿å­˜æˆåŠŸ")
    except Exception as e:
        print(f"æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        print("å°è¯•ä¿å­˜æ¨¡å‹åŸºæœ¬ä¿¡æ¯...")
        try:
            topic_info = topic_model.get_topic_info()
            topic_info.to_pickle(f"{args.output_dir}/topic_info.pkl")
            print("ä¸»é¢˜ä¿¡æ¯ä¿å­˜æˆåŠŸ")
        except Exception as e2:
            print(f"ä¸»é¢˜ä¿¡æ¯ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")
    
    print("ä¿å­˜ä¸»é¢˜ä¿¡æ¯...")
    try:
        df = topic_model.get_topic_info()
        df.to_csv(f"{args.output_dir}/topics.csv", index=False, encoding="utf-8")
        
        topics_dict = {}
        for topic_id in df['Topic'].values:
            if topic_id != -1:
                try:
                    topic_words = topic_model.get_topic(topic_id)
                    topics_dict[str(topic_id)] = topic_words
                except:
                    continue
        
        with open(f"{args.output_dir}/topic_words.json", "w", encoding="utf-8") as f:
            json.dump(topics_dict, f, ensure_ascii=False, indent=2)
            
        print("ä¸»é¢˜ä¿¡æ¯ä¿å­˜æˆåŠŸ")
        print(f"ä¸»é¢˜æ€»æ•°: {len(df)}")
        print(f"æœ‰æ•ˆä¸»é¢˜æ•°: {len([t for t in df['Topic'].values if t != -1])}")
        
    except Exception as e:
        print(f"ä¸»é¢˜ä¿¡æ¯ä¿å­˜å¤±è´¥: {e}")
    
    print(f"å¤„ç†å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {args.output_dir} ç›®å½•")
    print("\nç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨:")
    for file in os.listdir(args.output_dir):
        file_path = os.path.join(args.output_dir, file)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path) / 1024 / 1024  
            print(f"  ğŸ“„ {file} ({size:.2f} MB)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä½¿ç”¨BERTopicè¿›è¡Œä¸»é¢˜èšç±»åˆ†æ")
    parser.add_argument("--conv-file", type=str, required=False, help="è¾“å…¥çš„JSONå¯¹è¯æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--min-topic-size", type=int, default=32, help="æœ€å°ä¸»é¢˜å¤§å°")
    parser.add_argument("--embedding-file", type=str, default=None, help="é¢„è®¡ç®—çš„embeddingæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--post-process-conv", type=str, default=None, help="é¢„å¤„ç†çš„å¯¹è¯æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output-dir", type=str, default="topic_model_dir", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--first-n", type=int, default=None, help="åªå¤„ç†å‰Næ¡è®°å½•")
    parser.add_argument("--use-openai", action="store_true", help="æ˜¯å¦ä½¿ç”¨OpenAIè¿›è¡Œä¸»é¢˜æ ‡ç­¾ç”Ÿæˆ")
    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    try:
        run(args)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­ç¨‹åº")
        print("å·²ä¿å­˜çš„æ–‡ä»¶å¯ç”¨äºåç»­æ¢å¤")
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("\næ•…éšœæ’é™¤æç¤º:")
        print("1. æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("2. ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜å’ŒGPUèµ„æº")
        print("3. æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼ˆå¦‚æœä½¿ç”¨OpenAI APIï¼‰")
        print("4. æŸ¥çœ‹ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯è¿›è¡Œå…·ä½“è¯Šæ–­")